from collections import defaultdict
import logging
import urlparse
import xml.sax.saxutils as saxutils

from webtest.app import AppError 


logger = logging.getLogger('mcdp.spider')
logger.setLevel(logging.DEBUG)

class Spider():
    def __init__(self, get_maybe_follow, ignore=None):
        self.get_maybe_follow = get_maybe_follow
        if ignore is None:
            ignore = lambda _url, _parsed: False
        self.ignore = ignore
        self.queue = []
        self.skipped = set()
        self.failed = {} # url -> Exception
        self.visited = {} # url -> Response
        self.referrers = defaultdict(lambda: set()) # url -> url referred to
        
    def visit(self, url):
        self.queue.append(url)
                          
    def go(self, max_fails=None):
        while self.queue:
            self.step()
            if max_fails is not None:
                if len(self.failed) >= max_fails:
                    msg = 'Exiting because of max fails reached.'
                    logger.debug(msg)
                    break
            
    def step(self):
        url = self.queue.pop(-1)
        if url in self.visited:
            return
        o = urlparse.urlparse(url)
        
        if self.ignore(url, o):
            self.skipped.add(url)
            return
        
        logger.debug('requests %s ... ' % url)
        
        try:
            url2, res = self.get_maybe_follow(url)
        except (AppError) as e:
            logger.error('failed %s' % url)
            s = unicode(e).encode('utf8')
            self.failed[url] = saxutils.unescape(s)
            return
        
            
        if url2 != url:
            self.visited[url] = 'redirect to %s' % url2
            logger.debug('redirected %s -> %s' % (url, url2))
            
        self.visited[url2] = res
        
        if res.content_type == 'text/html':
            
            urls = list(find_links(res.html, url2))
            logger.debug('read %s %s: %d links' % (url2, res.status, len(urls)))
            for u in urls:
                p = urlparse.urlparse(u)
                invalid = False
                invalid = invalid or '../../../' in p.path
                invalid = invalid or '//' in p.path
                
                if invalid:
                    msg = 'We generated a URL that is weird: '
                    msg += '\n URL: %s ' % u
                    msg += '\n generated by: %s ' % url2
                    if url != url2:
                        msg += '\n redirected from: %s ' % url
                    raise ValueError(msg)
                self.queue.append(u)
                self.referrers[u].add(url2)
    
    def log_summary(self):
        logger.info('Visited: %d' % len(self.visited))
        logger.info('Skipped: %d' % len(self.skipped))
        if self.failed:
            logger.error('Failed: %d' % len(self.failed))
        for url in sorted(self.visited):
            logger.info('visisted %s' % url)
        # for url in sorted(self.skipped):
        # logger.debug('skipped %s' % url)
        for url in sorted(self.failed):
            logger.error('failed %s' % url)
            for r in self.referrers[url]:
                logger.error(' referred from %s' % r)
                u0 = list(self.referrers[url])[0]
                # logger.debug(indent(self.visited[u0].body, ' referrer page '))

                    
            logger.error(self.failed[url])

                
                
def find_links(html, url_base):
    '''
        Ignores "data:" urls in images.
    '''   
    def find(): 
        for link in html.select('link[href]'):
            yield link['href']
        for script in html.select('script[src]'):
            yield script['src']
        for img in html.select('img[src]'):
            if img['src'].startswith('data:'):
                continue
            yield img['src']
        for a in html.select('a[href]'):
            yield a['href']
    for url in find(): 
        yield urlparse.urljoin(url_base, url).encode('utf8')
